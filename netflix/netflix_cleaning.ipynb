{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do cleaning of Netflix dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.chdir('C:/Users/nikit/data_analysis/netflix/datasets')\n",
    "\n",
    "with open('netflix1.csv','r', encoding='utf8') as file:\n",
    "    netflix = pd.read_csv(file, index_col='show_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some funcs to make my life easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    null_mask = df.isin(['Not Given']).any(axis=1)\n",
    "    null_frame = df[null_mask]\n",
    "    df_not_null = df[null_mask == False]\n",
    "    \n",
    "    return null_frame, df_not_null\n",
    "\n",
    "def print_missing(df: pd.DataFrame) -> None:\n",
    "    print('Missing values:')\n",
    "    print(df.isin(['Not Given']).sum())\n",
    "    print('Total missing values:', df.isin(['Not Given']).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many rows are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial nulls:\n",
      "Missing values:\n",
      "type               0\n",
      "title              0\n",
      "director        2588\n",
      "country          287\n",
      "date_added         0\n",
      "release_year       0\n",
      "rating             0\n",
      "duration           0\n",
      "listed_in          0\n",
      "dtype: int64\n",
      "Total missing values: 2875\n"
     ]
    }
   ],
   "source": [
    "print('Initial nulls:')\n",
    "print_missing(netflix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's around 30% so I can't drop them right now. I'll try to treat them. \n",
    "\n",
    "- As first step, I will look for similarities in titles (in first two words)\n",
    "- Then, I will treat missing countries firtsly by similarities in type, director, and rating and secondly by director only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_frame, netflix_not_null = get_null(netflix)\n",
    "split_title = netflix_not_null['title'].str.replace(r'(\\S+)\\s(\\S+).*', r'\\1 \\2', regex=True)\n",
    "split_title = pd.DataFrame(split_title, columns=['title'])\n",
    "split_title.index.name = 'show_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got first two words of each title in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in null_frame.index.tolist():\n",
    "    row = null_frame.loc[id]\n",
    "    title = row['title'].split(' ')[:2]\n",
    "    title = ' '.join(title)\n",
    "    rep_id = split_title[split_title['title'] == title]\n",
    "    if not rep_id.empty:\n",
    "        rep_id = rep_id.index[0]\n",
    "        rep_row = netflix_not_null.loc[rep_id]\n",
    "        if row['director'] == 'Not Given':\n",
    "            netflix.loc[id, 'director'] = rep_row['director']\n",
    "        if row['country'] == 'Not Given':\n",
    "            netflix.loc[id, 'country'] = rep_row['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After treatment by similarities in titles:\n",
      "Missing values:\n",
      "type               0\n",
      "title              0\n",
      "director        2301\n",
      "country          235\n",
      "date_added         0\n",
      "release_year       0\n",
      "rating             0\n",
      "duration           0\n",
      "listed_in          0\n",
      "dtype: int64\n",
      "Total missing values: 2536\n"
     ]
    }
   ],
   "source": [
    "print('After treatment by similarities in titles:')\n",
    "print_missing(netflix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to treat about 300 missing directors and 50 missing countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After treatment by other similarities:\n",
      "Missing values:\n",
      "type               0\n",
      "title              0\n",
      "director        2301\n",
      "country          234\n",
      "date_added         0\n",
      "release_year       0\n",
      "rating             0\n",
      "duration           0\n",
      "listed_in          0\n",
      "dtype: int64\n",
      "Total missing values: 2535\n"
     ]
    }
   ],
   "source": [
    "null_frame, netflix_not_null = get_null(netflix)\n",
    "type_dir = netflix_not_null[['type', 'director', 'country']]\n",
    "# Treatment by other similarities\n",
    "for id in null_frame.index.tolist():\n",
    "    row = null_frame.loc[id]\n",
    "    if row['country'] == 'Not Given':\n",
    "        similar = type_dir[(type_dir['type'] == row['type'])&\n",
    "                            (type_dir['director'] == row['director'])]\n",
    "        if not similar.empty:\n",
    "            suggest = similar['country'].value_counts().idxmax()\n",
    "            netflix.loc[id, 'country'] = suggest\n",
    "\n",
    "print('After treatment by other similarities:')\n",
    "print_missing(netflix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not doing much now, bc those directors seem to have only one commitment in this dataset. So I'm not really able to do much about that without creating a ton of misleading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After treatment by director:\n",
      "Missing values:\n",
      "type               0\n",
      "title              0\n",
      "director        2301\n",
      "country          234\n",
      "date_added         0\n",
      "release_year       0\n",
      "rating             0\n",
      "duration           0\n",
      "listed_in          0\n",
      "dtype: int64\n",
      "Total missing values: 2535\n"
     ]
    }
   ],
   "source": [
    "null_frame, netflix_not_null = get_null(netflix)\n",
    "\n",
    "# Treatment by director\n",
    "for id in null_frame.index.tolist():\n",
    "    row = null_frame.loc[id]\n",
    "    if row['country'] == 'Not Given':\n",
    "        similar = netflix_not_null[netflix_not_null['director'] == row['director']]\n",
    "        if not similar.empty:\n",
    "            suggest = similar['country'].value_counts().idxmax()\n",
    "            netflix.loc[id, 'country'] = suggest\n",
    "\n",
    "print('After treatment by director:')\n",
    "print_missing(netflix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As last attempt I'll try to group data by date_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After treatment by date_added:\n",
      "Missing values:\n",
      "type               0\n",
      "title              0\n",
      "director        1457\n",
      "country          234\n",
      "date_added         0\n",
      "release_year       0\n",
      "rating             0\n",
      "duration           0\n",
      "listed_in          0\n",
      "dtype: int64\n",
      "Total missing values: 1691\n"
     ]
    }
   ],
   "source": [
    "for group, frame in netflix.groupby('date_added'):\n",
    "    null_frame, netflix_not_null = get_null(frame)\n",
    "    for id in null_frame.index.tolist():\n",
    "        row = null_frame.loc[id]\n",
    "        if row['director'] == 'Not Given':\n",
    "            similar = netflix_not_null[netflix_not_null['country'] == row['country']]\n",
    "            if not similar.empty:\n",
    "                suggest = similar['director'].value_counts().idxmax()\n",
    "                netflix.loc[id, 'director'] = suggest\n",
    "\n",
    "print('After treatment by date_added:')\n",
    "print_missing(netflix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastically! Let's check if there are weird values (e.g. directors with 100+ movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Given                           1457\n",
      "Charlie Haskell, Koichi Sakamoto      34\n",
      "Rajiv Chilaka                         21\n",
      "David Briggs                          20\n",
      "Alastair Fothergill                   19\n",
      "                                    ... \n",
      "Sandeep Reddy Vanga                    1\n",
      "Randall Lobb                           1\n",
      "Teng Huatao                            1\n",
      "Nzingha Stewart                        1\n",
      "Mozez Singh                            1\n",
      "Name: director, Length: 4528, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(netflix['director'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charlie Haskell and his friend are likely filmed all stuff about Power Rangers, check if there is evidence in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_id\n",
      "s7779    Power Rangers Super Megaforce: The Legendary B...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with open('netflix1.csv','r', encoding='utf8') as file:\n",
    "    _netflix = pd.read_csv(file, index_col='show_id')\n",
    "    print(_netflix.query('director == \"Charlie Haskell, Koichi Sakamoto\"')['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it's all right. Let's finally check Rajiv Chilaka, his major work is Chhota Beem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_id\n",
      "s420                         Chhota Bheem: Bheem vs Aliens\n",
      "s407                          Chhota Bheem - Neeli Pahaadi\n",
      "s408                                 Chhota Bheem & Ganesh\n",
      "s409                    Chhota Bheem & Krishna: Mayanagari\n",
      "s410     Chhota Bheem & Krishna: Pataliputra- City of t...\n",
      "s412                Chhota Bheem And The Crown of Valhalla\n",
      "s411                    Chhota Bheem And The Broken Amulet\n",
      "s416                              Chhota Bheem aur Krishna\n",
      "s413                  Chhota Bheem and the Incan Adventure\n",
      "s414                   Chhota Bheem and The ShiNobi Secret\n",
      "s415                              Chhota Bheem Aur Hanuman\n",
      "s417                   Chhota Bheem aur Krishna vs Zimbara\n",
      "s421                  Chhota Bheem: Dholakpur to Kathmandu\n",
      "s423                        Chhota Bheem: Journey to Petra\n",
      "s424                       Chhota Bheem: Master of Shaolin\n",
      "s425                     Chhota Bheem: The Rise of Kirmada\n",
      "s2718                Chhota Bheem and the Curse of Damyaan\n",
      "s6298                                            Bheemayan\n",
      "s6646                                Dragonkala Ka Rahasya\n",
      "s40                                           Chhota Bheem\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with open('netflix1.csv','r', encoding='utf8') as file:\n",
    "    _netflix = pd.read_csv(file, index_col='show_id')\n",
    "    print(_netflix.query('director == \"Rajiv Chilaka\"')['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No doubt about it. So I guess our dataset is cleaned to the highest achievable degree. So let's write it to new .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('netflix_clean.csv', 'wb') as file:\n",
    "    netflix.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('data_analytics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5fd812fb4fbb20649985952f396b781e66a19dd943b33b63b4b6da64efdfed4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
